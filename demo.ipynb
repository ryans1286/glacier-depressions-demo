{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-africa",
   "metadata": {},
   "source": [
    "## Import the data into a Pandas dataframe however you wish and take a look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ngozumpa-depressions_2019.csv', index_col = [0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-pepper",
   "metadata": {},
   "source": [
    "## It's a good idea to get a general idea of the general statistics of the data. What do you notice about these statistics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-first",
   "metadata": {},
   "source": [
    "## These data were calculated from a 1 meter resolution DEM. Therefore, it is unlikely that depressions ~1 meter represent actual features in the topography. Instead, these small depressions probably arise from noise in the data. Let's eliminate all the depressions smaller than 10 m$^2$ from the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['area'] >= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-cookie",
   "metadata": {},
   "source": [
    "## Histograms are a quick and easy way to look at how the data are distributed. Let's look at a histogram of depression areas. What do you notice about the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.area)\n",
    "plt.xlabel('Area', fontsize = 16)\n",
    "plt.ylabel('Abundance', fontsize = 16)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-pillow",
   "metadata": {},
   "source": [
    "## Looking back at the basic statistics, we see that the depression areas span more than 5 orders of magnitude. This is a good sign that we need to transform our data into log space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log10(df.area))\n",
    "plt.xlabel('Log(Area)', fontsize = 16)\n",
    "plt.ylabel('Abundance', fontsize = 16)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-rwanda",
   "metadata": {},
   "source": [
    "## Transforming the areas into log-space looks much nicer. Notice that the most abundant bin size (smallest depressions) has more than 1500 depressions in it, while the largest bin has very few depressions. Let's transform the number of depressions in each bin into log space as well. Now we have a histogram of depression areas in log-log space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log10(df.area), log = True)\n",
    "plt.xlabel('Log(Area)', fontsize = 16)\n",
    "plt.ylabel('Log(Abundance)', fontsize = 16)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-vacation",
   "metadata": {},
   "source": [
    "## This histogram looks linear in log-log space. That is strongly suggestive of a power law distribution. Histograms are not the best way to visualize power law distributions. Instead, we should try to create the cumulative probability density function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-birthday",
   "metadata": {},
   "source": [
    "## $$\\overline{F}(a) = P(a < a_{max})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-directive",
   "metadata": {},
   "source": [
    "## All this function is saying is we want to calculate the probability that a depression of size $a$ is smaller than the largest depression $a_{max}$. \n",
    "\n",
    "## Based on the histograms, we should expect this probability distribution to be a decreasing function. I.e., as depressions grow larger, the chance they're smaller than the biggest one should decrease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to calculate the probability distribution\n",
    "def normed_CPD(data):\n",
    "    #Lets sort the data from smallest to largest\n",
    "    data = np.sort(data)\n",
    "    \n",
    "    #Probability is just the number of outcomes we are interested in\n",
    "    #divided by the total number of possible outcomes\n",
    "    \n",
    "    #Let's create an array that stores the number of outcomes\n",
    "    # data[i] is the value we're interested in\n",
    "    # data is all the remaining values\n",
    "    # (data[i] < data).sum() counts the number of data points larger than data[i]\n",
    "    # for i in range(len(data)) repeats this calculation for all the possible data values\n",
    "    prob = [(data[i] < data).sum() for i in range(len(data))]\n",
    "    \n",
    "    #Finally, we calculate the probability by dividing by the maximum number of outcomes\n",
    "    return prob / max(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-blind",
   "metadata": {},
   "source": [
    "## Now let's calculate the cumulative probability distribution and plot in log-log space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(np.sort(df['area']), normed_CPD(df['area']))\n",
    "plt.xlabel('Area', fontsize = 16)\n",
    "plt.ylabel('Abundance', fontsize = 16)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-healing",
   "metadata": {},
   "source": [
    "## This distribution looks remarkably linear in log-log space. Our confidence should be growing that these data may be power law distributed.\n",
    "\n",
    "## It is not straightforward to calculate the slope of a probability distribution in log-log space using the least-squares regression method. Instead, we need to turn to the mathematicians to tell us what to do. \n",
    "\n",
    "## Newman et al. (2005) present an easy way to calculate the scaling exponent, based on some rigorous statistics that we will not go into here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-category",
   "metadata": {},
   "source": [
    "## $$ \\alpha = 1 + n \\left[ \\sum_{i=1}^{n} \\frac {x_i}{x_{min}} \\right]^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-elephant",
   "metadata": {},
   "source": [
    "## This may look scary, but it's pretty easy to code up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our code will be easier to understand if we define a function to do this for us\n",
    "def calc_alpha(data, low_cutoff = None, high_cutoff = None):\n",
    "    #Let's sort the data \n",
    "    data = np.sort(data)\n",
    "    \n",
    "    #These are checkpoints that choose values for parameters in case the user does not specify \n",
    "    if high_cutoff == None:\n",
    "        high_cutoff = max(data)\n",
    "    if low_cutoff == None:\n",
    "        low_cutoff = min(data)\n",
    "        \n",
    "    #We trim the data to just look at the range of values in which we are interested\n",
    "    data_cut = data[(data >= low_cutoff) & (data <= high_cutoff)]\n",
    "    \n",
    "    #This line does all the work for us\n",
    "    # 'n' is the length of our data set\n",
    "    # In the brackets of the equation above, \n",
    "    #   we just need to add up all the values in our data set and divide by the x_min value\n",
    "    # The rest is just following the order of operations\n",
    "    return 1 + len(data_cut) * (np.sum(np.log(data_cut / low_cutoff)))**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = calc_alpha(df['area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-nursing",
   "metadata": {},
   "source": [
    "## We can now use our approximation for alpha and a generalized form of the power law distribution to compare our distribution to a \"perfect\" power law distribution. See Clauset et al. (2009) if you're interested in the mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-absolute",
   "metadata": {},
   "source": [
    "## $$\\overline{F}(a) = \\left( \\frac{a}{a_{min}} \\right)^{-\\alpha + 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To plot our \"perfect\" power law, we need to create a dummy variable that spans\n",
    "# the same range as our data.\n",
    "temp = np.linspace(min(df['area']), max(df['area'])+100000, 5000)\n",
    "#Then we calculate the cumlative probability distribution using the function above. \n",
    "cpd = (temp/10)**-(alpha - 1)\n",
    "\n",
    "plt.loglog(np.sort(df['area']), normed_CPD(df['area']), linewidth = 3)\n",
    "plt.loglog(temp, 1.5*cpd, linestyle = '--', dashes = (3,2), linewidth = 3)\n",
    "plt.xlabel('Area', fontsize = 16)\n",
    "plt.ylabel('$\\\\overline{F}(A)$', fontsize = 16)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.annotate('$\\\\overline{F}(A) \\propto A^{1.67} $', xy = (.6, .65), xycoords ='axes fraction', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-peninsula",
   "metadata": {},
   "source": [
    "## Now let's take a look at some of the geometric scaling relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-estimate",
   "metadata": {},
   "source": [
    "## Perimeter-area scaling relationships tell us how the perimeter of a shape grows with increasing area. \n",
    "\n",
    "## $$ P \\propto A^{D/2} $$\n",
    "\n",
    "## Euclidean shapes should have $D=1$, but fractal shapes take on values greater than 1 but less than 2. \n",
    "\n",
    "## Shapes with $D>1$ are fractals. See Mandelbrot (1967) for a good introduction to fractals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "#Transform the data into log space to calculate the slope\n",
    "m, yint, pval, rval, se = linregress(np.log10(df['area']), np.log10(df['perimeter']))\n",
    "\n",
    "temp = np.linspace(10, 10**5, 10)\n",
    "\n",
    "plt.loglog(df['area'], df['perimeter'], 'o')\n",
    "\n",
    "#Don't forget to transform back into linear space to plot\n",
    "plt.loglog(temp, 10**yint*temp**m, linewidth = 4)\n",
    "plt.xlabel('Area', fontsize = 16)\n",
    "plt.ylabel('Perimeter', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-today",
   "metadata": {},
   "source": [
    "## How do we find the fractal dimension D? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2 * m\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-concrete",
   "metadata": {},
   "source": [
    "## Is there a scaling relationship for depth-area? Can you write it out mathematically? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the data into log space to calculate the slope\n",
    "m1, yint1, pval1, rval1, se1 = linregress(np.log10(df['area']), np.log10(df['depth']))\n",
    "\n",
    "temp = np.linspace(10, 10**5, 10)\n",
    "\n",
    "plt.loglog(df['area'], df['depth'], 'o')\n",
    "\n",
    "#Don't forget to transform back into linear space to plot\n",
    "plt.loglog(temp, 10**yint1*temp**m1, linewidth = 4)\n",
    "plt.xlabel('Area', fontsize = 16)\n",
    "plt.ylabel('Depth', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = m1 * 2\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-brazilian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-result",
   "metadata": {},
   "source": [
    "# Let's take a look at the code I wrote to create figures for my soon-to-be-published manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.colors as colors\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-danger",
   "metadata": {},
   "source": [
    "## Here are the functions I needed to perform my analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumul_prob(data):\n",
    "    data = np.sort(data)\n",
    "    return np.array([(data[i] <= data).sum() for i in range(len(data))])\n",
    "\n",
    "def normed_CPD(data):\n",
    "    data = np.sort(data)\n",
    "    prob = [(data[i] < data).sum() for i in range(len(data))]\n",
    "    return prob / max(prob)\n",
    "\n",
    "def calc_alpha(data, low_cutoff = None, high_cutoff = None):\n",
    "    data = np.sort(data)\n",
    "    if high_cutoff == None:\n",
    "        high_cutoff = max(data)\n",
    "    if low_cutoff == None:\n",
    "        low_cutoff = min(data)\n",
    "    data_cut = data[(data >= low_cutoff) & (data <= high_cutoff)]\n",
    "    return 1 + len(data_cut) * (np.sum(np.log(data_cut / low_cutoff)))**-1\n",
    "\n",
    "def calc_sigma(data, alpha, low_cutoff = None, high_cutoff = None):\n",
    "    data = np.sort(data)\n",
    "    if high_cutoff == None:\n",
    "        high_cutoff = max(data)\n",
    "    if low_cutoff == None:\n",
    "        low_cutoff = min(data)\n",
    "    data_cut = data[(data >= low_cutoff) & (data <= high_cutoff)]\n",
    "    return (alpha - 1) / np.sqrt(len(data_cut))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks_2019 = pd.read_csv('2019_Ngo-depressions_1m-borderdata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-buying",
   "metadata": {},
   "source": [
    "## I wanted to make a multiplot showing the results from all of my analyses. Since these analyses take a lot of steps, I first made a cell to complete all the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approximate amin, or define variable amin \n",
    "#alpha, amin, st, pval = powerlaw_bestFit(sinks_2019['area'], 10, 200, 10, 0.95)\n",
    "alpha = 1.67\n",
    "amin = 60\n",
    "#Calculate probability densities\n",
    "df = sinks_2019[sinks_2019['area'] >= 10]\n",
    "data = df['area']\n",
    "x2 = np.linspace(min(data), max(data)+100000, 5000)\n",
    "cpd = (amin / x2)**(alpha - 1)\n",
    "dataCPD = normed_CPD(data)\n",
    "\n",
    "#Calculate perimeter-area scaling relationship\n",
    "x3 = np.linspace(10, max(data), 10)\n",
    "aranges = np.linspace(10, 10**5)\n",
    "sqAreas = np.sqrt(data)\n",
    "perimeters = df['perimeter']\n",
    "restrict = df[df['area'] >= amin]\n",
    "area_r = restrict['area']\n",
    "perimeter_r = restrict['perimeter']\n",
    "D, b, r, p, se = linregress(np.log10(np.sqrt(area_r)), np.log10(perimeter_r))\n",
    "tempfrac = df.sample(frac=0.2, random_state = 1)\n",
    "perimeters = tempfrac['perimeter']\n",
    "areasfrac = tempfrac['area']\n",
    "\n",
    "#Calculate depression depth kernel densities\n",
    "slope_test = sinks_2019[sinks_2019['area'] >= 10]\n",
    "areas = np.log10(slope_test['area'])\n",
    "depth = np.log10(slope_test['_max'])\n",
    "areamin = areas.min()\n",
    "areamax = areas.max()\n",
    "dmin = depth.min()\n",
    "dmax = depth.max()\n",
    "xd, yd = np.mgrid[areamin:areamax:200j, dmin:dmax:200j]\n",
    "positionsd = np.vstack([xd.ravel(), yd.ravel()])\n",
    "valuesd = np.vstack([areas, depth])\n",
    "kerneld = stats.gaussian_kde(valuesd)\n",
    "zd = np.reshape(kerneld(positionsd).T, yd.shape)\n",
    "tempdepths = slope_test.sample(frac=0.2, random_state = 1)\n",
    "areas = np.log10(tempdepths['area'])\n",
    "depth = np.log10(tempdepths['_max'])\n",
    "\n",
    "#Calculate area-depth scaling relationships\n",
    "tempdf = slope_test\n",
    "tempdf = tempdf[tempdf['area'] >= 50]\n",
    "tempareas = np.log10(tempdf['area'])\n",
    "tempdepth = np.log10(tempdf['_max'])\n",
    "slope, yint, rval, pval, se = linregress(tempareas, tempdepth)\n",
    "\n",
    "#Calculate perimeter slope kernel densities\n",
    "slope_test = slope_test[slope_test['slope_medi'].notna()]\n",
    "area = np.log10(slope_test['area'])\n",
    "median = slope_test['slope_medi']\n",
    "medMin = median.min()\n",
    "medMax = median.max()\n",
    "x, y = np.mgrid[areamin:areamax:200j, medMin:medMax:200j]\n",
    "positions = np.vstack([x.ravel(), y.ravel()])\n",
    "values = np.vstack([area, median])\n",
    "kernel = stats.gaussian_kde(values)\n",
    "z = np.reshape(kernel(positions).T, y.shape)\n",
    "tempdensities = slope_test.sample(frac=0.2, random_state = 1)\n",
    "area = np.log10(tempdensities['area'])\n",
    "median = tempdensities['slope_medi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.linspace(np.log10(50), areamax-1, 100)\n",
    "\n",
    "#CREATE A MULTIPLOT USING THE fig, ax = plt.subplots() NOTATION\n",
    "fig, ax = plt.subplots(2,2, figsize = (9,6))\n",
    "\n",
    "#Plot the depressions probability distribution\n",
    "ax[0,0].loglog(np.sort(data), dataCPD, linewidth = 3, color = 'dodgerblue')\n",
    "ax[0,0].loglog(x2, 0.5*cpd, linestyle = '--', dashes = (3,2), linewidth = 2, label = '$\\\\overline{F}(A) \\propto A^{-1.66} $', color = 'k')\n",
    "ax[0,0].legend(loc = (.4, .65), fontsize = 16, framealpha = 1, handlelength = 0, frameon = False)\n",
    "ax[0,0].set_xlim([10**0.8, 10**5.6])\n",
    "ax[0,0].set_ylabel('$\\\\overline{F}(A)$', fontsize = 14)\n",
    "ax[0,0].get_xaxis().set_ticks([10**1, 10**3, 10**5])\n",
    "ax[0,0].get_yaxis().set_ticks([10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
    "ax[0,0].tick_params('both', length = 10, width = 1.2, which = 'major', labelsize = 12)\n",
    "ax[0,0].tick_params('both', length = 0, width = 1, which = 'minor')\n",
    "ax[0,0].annotate('(a)', xy = (-.3,1.1), xycoords = 'axes fraction', fontsize = 16)\n",
    "\n",
    "#Plot fractal dimensions (area-perimeter scaling relationship)\n",
    "ax[0,1].loglog(areasfrac, perimeters, 'o', markersize = 1, alpha = 1, color = 'forestgreen')\n",
    "ax[0,1].loglog(x3, 10**b*x3**(D/2), linestyle = '--', dashes = (3,2), linewidth = 2, label = '$P \\propto A^{1.16}$', color = 'k')\n",
    "ax[0,1].loglog(x3, 3.5*x3**0.5, linestyle = ':', linewidth = 1, color = 'k')\n",
    "ax[0,1].legend(loc = (.2, .65), fontsize = 16, handlelength = 0, framealpha = 1, frameon = False)\n",
    "ax[0,1].set_xlim([10**0.8, 10**5.6])\n",
    "ax[0,1].set_ylabel('Perimeter [m]', fontsize = 14)\n",
    "ax[0,1].get_xaxis().set_ticks([10**1, 10**3, 10**5])\n",
    "ax[0,1].tick_params('both', length = 10, width = 1.2, which = 'major', labelsize = 12)\n",
    "ax[0,1].tick_params('both', length = 0, width = 1, which = 'minor')\n",
    "ax[0,1].annotate('(b)', xy = (-0.3,1.1), xycoords = 'axes fraction', fontsize = 16)\n",
    "\n",
    "#Plot the kernel density estimation for the depth-area scaling relationship\n",
    "ax[1,0].imshow(np.rot90(zd), cmap = 'gist_heat_r', extent = [areamin, areamax, dmin, dmax], aspect = 'auto', norm = colors.Normalize(vmin = zd.min(), vmax = np.percentile(zd, q = 100)))\n",
    "ax[1,0].plot(areas, depth, 'ko', markersize = 0.5, alpha = 1)\n",
    "ax[1,0].plot(temp, slope * temp + yint+0.5, '--', c = 'k', linewidth = 2, label = '$d \\propto A^{0.42}$') \n",
    "ax[1,0].get_xaxis().set_ticks([1,3,5])\n",
    "ax[1,0].get_yaxis().set_ticks([-1, 0, 1])\n",
    "ax[1,0].set_xticklabels([\"$10^1$\",\"$10^3$\",\"$10^5$\"], fontsize = 12)\n",
    "ax[1,0].set_yticklabels([\"$0.1$\",\"$1$\",\"$10$\"], fontsize = 12)\n",
    "ax[1,0].set_xlim([0.8, 5.6])\n",
    "ax[1,0].set_ylabel('Depth [m]', fontsize = 14)\n",
    "ax[1,0].tick_params(labelsize = 12)\n",
    "ax[1,0].tick_params('both', length = 10, width = 1.2, which = 'major')\n",
    "ax[1,0].legend(loc = (0,0.7), handlelength = 0, fontsize = 16, frameon = False)\n",
    "ax[1,0].set_xlabel('Depression Area [m$^2$]', fontsize = 14)\n",
    "ax[1,0].annotate('(c)', xy = (-.3,1.1), xycoords = 'axes fraction', fontsize = 16)\n",
    "\n",
    "#Plot perimeter slopes kernel density estimation\n",
    "ax[1,1].imshow(np.rot90(z), cmap = 'bone_r', extent = [areamin, areamax, medMin, medMax], aspect = 'auto', norm = colors.Normalize(vmin = z.min(), vmax = np.percentile(z, q = 100)))\n",
    "ax[1,1].plot(area, median, 'o', color = 'k', alpha = 1, markersize = 0.5)\n",
    "ax[1,1].get_xaxis().set_ticks([1,3,5])\n",
    "ax[1,1].set_ylim(0, 39)\n",
    "ax[1,1].set_xticklabels([\"$10^1$\",\"$10^3$\",\"$10^5$\"], fontsize = 12)\n",
    "ax[1,1].axhline(27, c = 'k', linestyle = ':', linewidth = 1.5, label = '$\\\\theta_{c} = 27^\\circ$')\n",
    "ax[1,1].set_xlim([0.8, 5.6])\n",
    "ax[1,1].set_xlabel('Depression Area [m$^2$]', fontsize = 14)\n",
    "ax[1,1].set_ylabel('Median Slope [$\\circ$]', fontsize = 14)\n",
    "ax[1,1].tick_params(labelsize = 12)\n",
    "ax[1,1].tick_params('both', length = 10, width = 1.2, which = 'major')\n",
    "ax[1,1].annotate('(d)', xy = (-.3,1.1), xycoords = 'axes fraction', fontsize = 16)\n",
    "ax[1,1].legend(loc = (0.65, 0.7), handlelength = 0, fontsize = 16, frameon = False)\n",
    "sns.despine(offset = [5,5])\n",
    "fig.subplots_adjust(hspace = 0.45, wspace = 0.55)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-setting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
